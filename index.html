<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Enhancing Construction Site Safety: Advanced Deep Learning Approaches for Accurate Helmet Detection</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Enhancing Construction Site Safety: Advanced Deep Learning Approaches for Accurate Helmet Detection</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="LINK TO LEFKI IOANNA PANAGIOTOU'S PROFILE" target="_blank">Lefki Ioanna Panagiotou</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="LINK TO ZACHARIAS IOSIF SIFAKIS' PROFILE" target="_blank">Zacharias Iosif Sifakis</a><sup>*</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Wisconsin - Madison<br>ECE766 - Final Project (Spring 2024)</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                    </div>

                  

                   

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/EleannaPanagiotou/ECE766-HelmetDetection" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Google slides Link -->
                <span class="link-block">
                  <a href="https://docs.google.com/presentation/d/10eYvg_7GoZW7Hcm6mV9vxDlIlwcodQM2UQ8ESOhssMg/edit?usp=sharing" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-google"></i>
                  </span>
                  <span>Google Slides</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Image before Abstract -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/intro_img.png" alt="Descriptive Alt Text" style="width:100%; height:auto;">
      <h2 class="subtitle has-text-centered">
        Sample images from the Hard Hat worker dataset <a href="#ref-5">[5]</a>.
      </h2>
    </div>
  </div>
</section>
<!-- End Image before Abstract -->

<!-- Introduction and Motivation -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction and Motivation</h2>
        <div class="content has-text-justified">
          <p>
            The construction industry is known for its high-risk environments, with workers exposed to various hazards, including falling objects, which can lead to severe head injuries or even fatalities. The critical nature of these accidents, particularly head injuries resulting from the improper use of safety helmets, underscores a pressing concern for worker safety. In 2012 alone, the construction sector witnessed over 65,000 head injuries requiring days away from work, with head injuries culminating in 1,020 fatalities, according to the 2015 edition of the National Safety Council's "Injury Facts" chartbook <a href="#ref-6">[6]</a>. Moreover, out of the total of 391 people with body parts injured in the construction industry in 2016, 161 people (41.2%) were injured in the head, according to the Korea Occupational Safety and Health Agency <a href="#ref-9">[9]</a>.
          </p>
          <p>
            In response to this alarming issue, this project aims to find the best automatic safety helmet detection system that will mitigate workplace accidents, by leveraging the advanced capabilities of deep learning models in object detection. Many architectures can be used to solve this object detection problem. In this research, we investigate various state-of-the-art models, including YOLOv5 <a href="#ref-7">[7]</a>, YOLOv8 <a href="#ref-8">[8]</a>, and Vision Transformers (ViT) <a href="#ref-3">[3]</a>, (specifically the Detection Transformer (DETR), to ascertain their applicability and effectiveness on our dataset.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Introduction and Motivation -->

  <!-- Prior Work Section -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Prior Work on Safety Helmet Detection</h2>
        <div class="content">
          <p>Several prior studies in safety helmet detection on construction sites have used sensor-based methods and some algorithmic-based approaches. In <a href="#ref-1">[1]</a>, a combination of the Zigbee and Radio Frequency Identification (RFID) technologies was used to detect personal protective equipment (PPE) in workers. In <a href="#ref-4">[4]</a>, the authors combined Haar-like features and Circle Hough Transform (CHT) techniques for helmet detection. Also, in <a href="#ref-11">[11]</a>, an automatic helmet detection method using Histograms of Oriented Gradient (HOG) features and the CHT technique was proposed.</p>
          <p>More recent studies have focused on using deep learning techniques, particularly YOLO architectures, for safety helmet detection. In <a href="#ref-13">[13]</a>, different YOLO architectures were compared, with YOLOv5x providing the best precision. In <a href="#ref-11">[11]</a>, YOLOv3 with Gaussian blurring was used to address data imbalance issues.</p>
          <p>However, challenges remain, especially in detecting helmets in scenes with multiple workers and when the worker is not facing the camera. These insights underscore the need for ongoing research to refine deep learning models further, ensuring their effectiveness in real-world construction site environments and ultimately enhancing worker safety through improved automated detection systems. This is why we will also apply the Vision Transformer (ViT) architecture in the dataset, which has never been applied before.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Dataset Description Section -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset Description</h2>
        <div class="content">
          <p>
            The dataset we used for this project is the Hard Hat worker image dataset published by MakeML <a href="#ref-5">[5]</a>. This dataset is specifically designed for the detection of hard helmets used in the construction industry. It contains 5000 images, each accompanied by bounding box annotation files. Initially, the dataset includes three classes: Helmet, Person, and Head with more than 26000 total instances. 
          </p>
          <figure>
            <img src="static/images/data_description.jpg" alt="Histogram of the three classes in the dataset" style="width: 40%;">
            <figcaption>Figure 1: Histogram of the three classes in dataset.</figcaption>
          </figure>
          <h3 class="title is-4">Data Pre-processing</h3>
          <p>
            As we can observe from the Figure 1 above, the dataset is highly imbalanced. Most of the instances (approximately 15000) refer to the 'helmet' class while only about 5000 instances are of the 'head' class and less than 1000 instances are of the 'person' class. So, for this research, we only focus on two classes: 'Helmet' and 'Head'. The decision to focus only on these two classes is also based on the project’s primary goal of detecting whether workers are wearing their safety helmets correctly. By identifying instances of "Head" without a corresponding "Helmet," the system can alert supervisors to potential safety violations, allowing for prompt corrective action. Furthermore, in order to run  the YOLO experiments, we reshaped the images into (640x640) to comply with the YOLO input size requirements. Finally, we split our data into 3 sets (training, validation, and test sets) with 3000, 1000, and 1000 images respectively.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Dataset Description Section -->


  
<!-- Method Section -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content">
          
          <!-- YOLO Subsection -->
          <h3 class="title is-4">YOLO</h3>
          <p>
            For this project, we trained the YOLOv5 and YOLOv8 models. These are state-of-the-art object detection architectures that have gained significant popularity due to their efficiency and accuracy. These architectures share several key components that contribute to their exceptional performance:
          </p>
          <ol>
            <li>
              <strong>Backbone:</strong> Both YOLOv5 and YOLOv8 employ a modified version of the CSPNet (Cross Stage Partial Network) as their backbone for feature extraction. This design choice helps reduce the model's complexity while maintaining its effectiveness in capturing essential features from the input images.
            </li>
            <li>
              <strong>Neck:</strong> The PANet (Path Aggregation Network) serves as the neck in these architectures. Its primary function is to aggregate feature maps from different scales, enabling the model to detect objects of various sizes more effectively. The neck plays a crucial role in enhancing the model's ability to handle scale variations.
            </li>
            <li>
              <strong>Head:</strong> The detection head is responsible for predicting bounding boxes, objectness scores, and class probabilities. In YOLOv5, anchor boxes are utilized to improve detection accuracy. However, YOLOv8 adopts an anchor-free split Ultralytics head, which contributes to better accuracy and a more efficient detection process compared to anchor-based approaches.
            </li>
            <li>
              <strong>Scalability:</strong> Both versions of YOLO offer multiple model sizes (nano, small, medium, large, extra large) according to the number of parameters, allowing users to choose a model that best suits their specific requirements in terms of speed and accuracy. This scalability makes YOLO adaptable to various application domains and computational constraints.
            </li>
          </ol>
          <figure>
            <img src="static/images/yolo.png" alt="Visual representation of YOLO model architecture" style="width:100%; max-width:600px; height:auto;">
            <figcaption>Figure 2: Visual representation of YOLO model architecture.</figcaption>
          </figure>
          <p>
            Here, we focus on the YOLOv5n and YOLOv8n variants due to limited computation resources. 
          </p>

          <!-- DETR Subsection -->
          <h3 class="title is-4">DETR</h3>
          <p>
             addition to YOLO architectures, we also explored the use of DETR (Detection Transformer), a novel architecture that approaches object detection with a set-based global loss and uses transformers. DETR was proposed by the Facebook AI Research team in 2020 and eliminates the need for many hand-designed components like anchor generation and non-maximum suppression that YOLO relies on. This method has not been used for this specific dataset, to the best of our knowledge, and based on our results it can become one of the best approaches. It consists of 3 main components:
          </p>
          <ol>
            <li>
              <strong>CNN Backbone:</strong> DETR utilizes the pretrained ResNet50 for feature extraction. 
            </li>
            <li>
              <strong>Encoder - Decoder:</strong> These layers produce the final set of predicted class labels and bounding boxes through multiple multi-head self-attention and decoder-encoder attention.
            </li>
            <li>
              <strong>Feed Forward network:</strong>  for the final detection prediction.
            </li>
          </ol>
          <p>
            The exploration of DETR is aimed at understanding its potential for providing more scalable and efficient solutions in environments that require handling a large variety of object sizes and complex scenes.
          </p>
          <figure>
            <img src="static/images/detr.png" alt="Visual representation of DETR model architecture" style="width:100%; max-width:900px; height:auto;">
            <figcaption>Figure 3: Visual representation of DETR model architecture.</figcaption>
          </figure>

          The model is trained on the COCO dataset which contains 80 classes, not including ours. Therefore, the need to fine-tune the model to our dataset arose.


          <!-- Evaluation Metrics Subsection -->
          <h3 class="title is-4">Evaluation Metrics</h3>
          <p>We evaluate the performance of the models using metrics such as precision, recall, and mean average precision (mAP), each serving a distinct role in assessing the effectiveness of the helmet detection models. More specifically,</p>
          <ul>
            <li><strong>Precision:</strong> measures the proportion of true positive helmet predictions among all positive predictions, ensuring the model minimizes false alarms that could cause unnecessary interruptions on construction sites.</li>
            <li><strong>Recall:</strong> evaluates the model's ability to detect all actual helmets present, even in challenging conditions, which is vital for not missing any helmets that could compromise worker safety.</li>
            <li><strong>Mean Average Precision (mAP):</strong> provides a comprehensive measure of the model's quality by averaging precision values at different recall levels, enabling evaluation of its effectiveness in accurately identifying helmets under diverse and challenging real-world conditions.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Method Section -->

<!-- Results Section -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content">
          
          <!-- Main Results -->
          <p>The main results from our early experiments using YOLOv5 and YOLOv8 include the confusion matrices, the PR curves, the precision/recall/mAP, and the predictions for the test dataset.</p>
          <figure>
            <img src="static/images/confusion_matrix_test_v5.png" alt="Confusion matrix for the test dataset using YOLOv5 (true vs. predicted labels)" style="width:70%; max-width:600px; height:auto;">
            <figcaption>Figure 4: Confusion matrix for the test dataset using YOLOv5 (true vs. predicted labels)</figcaption>
          </figure>
          <figure>
            <img src="static/images/confusion_matrix_normalized_test_v8.png" alt="Confusion matrix for the test dataset using YOLOv8 (true vs. predicted labels)" style="width:70%; max-width:600px; height:auto;">
            <figcaption>Figure 5: Confusion matrix for the test dataset using YOLOv8 (true vs. predicted labels)</figcaption>
          </figure>
          <figure>
            <img src="static/images/PR_curve_test_v5.png" alt="Precision-Recall curve (PR Curve) for the test dataset using YOLOv5" style="width:50%; max-width:400px; height:auto;">
            <figcaption>Figure 6: Precision-Recall curve (PR Curve) for the test dataset using YOLOv5</figcaption>
          </figure>
          <figure>
            <img src="static/images/PR_curve_test_v8.png" alt="Precision-Recall curve (PR Curve) for the test dataset using YOLOv8" style="width:50%; max-width:400px; height:auto;">
            <figcaption>Figure 7: Precision-Recall curve (PR Curve) for the test dataset using YOLOv8</figcaption>
          </figure>
          <table class="table is-striped is-fullwidth">
            <caption>Table 1: Model Evaluation Metrics</caption>
            <thead>
              <tr>
                <th>Model</th>
                <th>Class</th>
                <th>Images</th>
                <th>Instances</th>
                <th>Precision</th>
                <th>Recall</th>
                <th>mAP</th>
              </tr>
            </thead>
            <tbody>
              <!-- Table rows -->
              <tr>
                <td>YOLOv5x</td>
                <td>all</td>
                <td>1000</td>
                <td>-</td>
                <td>0.910</td>
                <td>0.890</td>
                <td>-</td>
              </tr>
              <tr>
                <td rowspan="3">YOLOv5n</td>
                <td>all</td>
                <td>1000</td>
                <td>4888</td>
                <td>0.903</td>
                <td>0.869</td>
                <td>0.925</td>
              </tr>
              <tr>
                <td>helmet</td>
                <td>1000</td>
                <td>3759</td>
                <td>0.934</td>
                <td>0.872</td>
                <td>0.944</td>
              </tr>
              <tr>
                <td>head</td>
                <td>1000</td>
                <td>1129</td>
                <td>0.872</td>
                <td>0.865</td>
                <td>0.906</td>
              </tr>
              <tr>
                <td rowspan="3">YOLOv8n</td>
                <td>all</td>
                <td>1000</td>
                <td>4888</td>
                <td>0.918</td>
                <td>0.895</td>
                <td>0.944</td>
              </tr>
              <tr>
                <td>helmet</td>
                <td>1000</td>
                <td>3759</td>
                <td>0.934</td>
                <td>0.907</td>
                <td>0.961</td>
              </tr>
              <tr>
                <td>head</td>
                <td>1000</td>
                <td>1129</td>
                <td>0.901</td>
                <td>0.882</td>
                <td>0.926</td>
              </tr>
            <tr>
              <td rowspan="3">DETR</td>
              <td>all</td>
              <td>1000</td>
              <td>4888</td>
              <td>0.927</td>
              <td>0.903</td>
              <td>0.952</td>
            </tr>
            <tr>
              <td>helmet</td>
              <td>1000</td>
              <td>3759</td>
              <td>0.941</td>
              <td>0.912</td>
              <td>0.971</td>
            </tr>
            <tr>
              <td>head</td>
              <td>1000</td>
              <td>1129</td>
              <td>0.918</td>
              <td>0.899</td>
              <td>0.943</td>
            </tr>
          </tbody>
        </table>

        <table class="table is-striped is-fullwidth">
           <caption>Table 2: Training Configuration</caption>
            <thead>
              <tr>
                <th>Model</th>
                <th># of Parameters (M)</th>
                <th>Optimizer</th>
                <th># of Epochs</th>
                <th>Learning rate</th>
                <th>Batch Size</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>YOLOv5n</td>
                <td>1.9</td>
                <td>SGD</td>
                <td>50</td>
                <td>0.01</td>
                <td>32</td>
              </tr>
              <tr>
                <td>YOLOv8n</td>
                <td>3.2</td>
                <td>Adam</td>
                <td>50</td>
                <td>0.001</td>
                <td>32</td>
              </tr>
              <tr>
                <td>DETR</td>
                <td>41.3</td>
                <td>AdamW</td>
                <td>30</td>
                <td>0.01</td>
                <td>4</td>
              </tr>
              <tr>
                <td>YOLOv5x</td>
                <td>86.7</td>
                <td>SGD</td>
                <td>50</td>
                <td>0.001</td>
                <td>32</td>
              </tr>
            </tbody>
          </table>

          <figure>
            <img src="static/images/val_batch2_pred_v5.jpg" alt="Predictions for test batch of 16 images using YOLOv5" style="width:70%; max-width:900px; height:auto;">
            <figcaption>Figure 8: Predictions for test batch of 16 images using YOLOv5</figcaption>
          </figure>
          <figure>
            <img src="static/images/val_batch2_pred_v8.jpg" alt="Predictions for test batch of 16 images using YOLOv8" style="width:70%; max-width:900px; height:auto;">
            <figcaption>Figure 9: Predictions for test batch of 16 images using YOLOv8</figcaption>
          </figure>
        <!-- Comparative Predictions Subsection -->
          <section>
            <h3 class="title is-4">Comparative Predictions</h3>
            <figure>
              <img src="static/images/omp1.png" alt="Description for Additional Image 1" style="width:70%; max-width:2800px; height:auto;">
              <figcaption>Figure 10: Comparative Predictions from Transformer (on the left) and YOLOv5n (on the right) in a very clear image. Both models accurately detect both classes, with the transformer model exhibiting notably higher confidence levels compared to the YOLO model.</figcaption>
            </figure>
            <figure>
              <img src="static/images/comp2.png" alt="Description for Additional Image 2" style="width:70%; max-width:2800px; height:auto;">
              <figcaption>Figure 11: Testing Distant Instances with Transformer (on the left) and YOLOv8n (on the right). In this image, all instances are far from the camera, which is really challenging for the models. Here, not only do we observe a difference in the models' confidence levels, but there is also a noticeable discrepancy in accuracy. The YOLO approach misclassifies a red region in the center as a helmet and fails to detect an actual helmet in the upper part of the image.</figcaption>
            </figure>
            <figure>
              <img src="static/images/comp3.png" alt="Description for Additional Image 3" style="width:70%; max-width:2800px; height:auto;">
              <figcaption>Figure 12: Performance Comparison in Low-Light Conditions. This image, challenging due to its darkness, showcases predictions from the Transformer (on the right) and the YOLOv8n (on the left). The transformer model displays high confidence and accurately classifies all objects without any misclassifications. In contrast, the YOLO model, while still effective, shows lower confidence and incorrectly classifies a truck, illustrating distinct performance differences under low-light conditions.</figcaption>
            </figure>
          </section>
          <!-- End of Comparative Predictions Subsection -->
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Results Section -->

  <!-- Discussion Subsection -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Discussion</h2>
        <div class="content">
          <p>Our experimental analysis involved three advanced deep learning models—YOLOv5, YOLOv8, and DETR—each tested under various conditions and configurations to evaluate their effectiveness in helmet detection on construction sites. The performance metrics precision, recall, and mean average precision (mAP) provided comprehensive insights into each model's capability.

          <p>YOLOv5n and YOLOv8n demonstrated robust performance across all categories, with YOLOv8n showing a slight edge in precision and recall, especially in the 'helmet' and 'head' classes. This is indicative of YOLOv8n's improved architecture, which utilizes an anchor-free split Ultralytics head, enhancing detection accuracy and processing efficiency. Specifically, YOLOv8n reached a precision of 0.934 and a recall of 0.907 for helmets, reflecting its strong suitability for real-time safety applications where detecting safety gear accurately is critical.</p>

          <p>DETR, employing a transformer-based approach, outperformed both YOLO models in terms of precision and recall in low-light conditions and complex scenarios, as highlighted by the dark image test. DETR’s highest scores were in the 'helmet' category with a precision of 0.941 and a recall of 0.912. Its architecture allows it to manage challenging detection tasks better, thanks to its global view of the image and the elimination of many hand-designed components of conventional detectors.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Discussion Subsection -->


<!-- References Section -->
<section class="section" id="References">
  <div class="container is-max-desktop content">
    <h2 class="title">References</h2>
    <ol>
      <li id="ref-1">Santiago Barro-Torres et al. “Real-time personal protective equipment monitoring system”. In: Computer Communications 36.1 (2012), pp. 42–50. DOI: <a href="https://doi.org/10.1016/j.comcom.2012.01.005">10.1016/j.comcom.2012.01.005</a>. URL: <a href="https://www.sciencedirect.com/science/article/pii/S0140366412000060">https://www.sciencedirect.com/science/article/pii/S0140366412000060</a>.</li>
      <li id="ref-2">Nicolas Carion et al. “End-to-end object detection with transformers”. In: European conference on computer vision. Springer. 2020, pp. 213–229.</li>
      <li id="ref-3">Alexey Dosovitskiy et al. “An image is worth 16x16 words: Transformers for image recognition at scale”. In: arXiv preprint arXiv:2010.11929 (2020).</li>
      <li id="ref-4">Pathasu Doungmala and Katanyoo Klubsuwan. “Helmet Wearing Detection in Thailand Using Haar Like Feature and Circle Hough Transform on Image Processing”. In: 2016 IEEE International Conference on Computer and Information Technology (CIT). 2016, pp. 611–614. DOI: 10.1109/CIT.2016.87.</li>
      <li id="ref-5">Hard Hat Dataset. Make ML. URL: <a href="https://www.kaggle.com/datasets/andrewmvd/hard-hat-detection/data">https://www.kaggle.com/datasets/andrewmvd/hard-hat-detection/data</a>.</li>
      <li id="ref-6">HexArmor. “The hard truth about safety helmet injuries and statistics”. In: (2019). URL: <a href="https://www.hexarmor.com/posts/the-hard-truth-about-safety-helmet-injuries-and-statistics">https://www.hexarmor.com/posts/the-hard-truth-about-safety-helmet-injuries-and-statistics</a>.</li>
      <li id="ref-7">Glenn Jocher. Ultralytics YOLOv5. Version 7.0. 2020. DOI: 10.5281/zenodo.3908559. URL: <a href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a>.</li>
      <li id="ref-8">Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralytics YOLOv8. Version 8.0.0. 2023. URL: <a href="https://github.com/ultralytics/ultralytics">https://github.com/ultralytics/ultralytics</a>.</li>
      <li id="ref-9">Sung Hun Kim et al. “Safety helmet wearing management system for construction workers using three-axis accelerometer sensor”. In: Applied Sciences 8.12 (2018), p. 2400.</li>
      <li id="ref-10">Tsung-Yi Lin et al. “Microsoft coco: Common objects in context”. In: Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. Springer. 2014, pp. 740–755.</li>
      <li id="ref-11">Shaoqing Ren et al. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”. In: Advances in Neural Information Processing Systems. Ed. by C. Cortes et al. Vol. 28. Curran Associates, Inc., 2015. URL: <a href="https://proceedings.neurips.cc/paper_files/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf</a>.</li>
      <li id="ref-12">Abu H. M. Rubaiyat et al. “Automatic Detection of Helmet Uses for Construction Safety”. In: 2016 IEEE/WIC/ACM International Conference on Web Intelligence Workshops (WIW). 2016, pp. 135–142. DOI: 10.1109/WIW.2016.045.</li>
      <li id="ref-13">Zijian Wang et al. “Fast personal protective equipment detection for real construction sites using deep learning approaches”. In: Sensors 21.10 (2021), p. 3478.</li>
    </ol>
  </div>
</section>
<!-- End References Section -->



  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
